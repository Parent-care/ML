# -*- coding: utf-8 -*-
"""notebookfinalcapstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zz7m12X8KqC4RwrXf1z37dQOr-jo8u2F

## Import Library
"""

# Import Library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import requests
import string
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import joblib

"""## Load Data dan Data Understanding"""

# Load all datasets
train      = pd.read_csv("train_translated.csv")
prestasi   = pd.read_csv("parenting_prestasi.csv")
graduation = pd.read_csv("graduation_rate.csv")

# Preview the first rows
train_preview = train.head()
prestasi_preview = prestasi.head()
graduation_preview = graduation.head()

train_info = train.info()
prestasi_info = prestasi.info()
graduation_info = graduation.info()

# Show shape of each dataset
shapes = {
    "train_translated": train.shape,
    "parenting_prestasi": prestasi.shape,
    "graduation_rate": graduation.shape
}

(train_preview, prestasi_preview, graduation_preview, shapes)

"""## Insight
### 1. train_translated.csv
- **Jumlah data: 3512 baris, 4 kolom**
- Kolom penting:
  - `Context_Indo`: Teks pertanyaan atau curhat dalam Bahasa Indonesia
  - `Response_Indo`: Teks jawaban/respons dalam Bahasa Indonesia
- Tujuan potensial: Digunakan sebagai data percakapan atau basis klasifikasi berdasarkan isi curhatan untuk memetakan ke tipe parenting.

### 2. parenting_prestasi.csv
- **Jumlah data: 480 baris, 17 kolom**
- Berisi: Data performa belajar siswa dan keterlibatan orang tua.
- Fitur penting:
  - `raisedhands`, `VisitedResources`, `Discussion`: Indikator keaktifan siswa.
  - `ParentAnsweringSurvey`, `ParentschoolSatisfaction`: Indikator kepedulian orang tua.
- `Class`: Label kelas performa siswa (L, M, H).

### 3. graduation_rate.csv
- **Jumlah data: 1000 baris, 7 kolom**
- Berisi: Data kelulusan berdasarkan latar belakang orang tua dan capaian akademik.
- Fitur penting:
  - parental level of education, parental income
  - high school gpa, college gpa
  - years to graduate: Durasi studi sebagai indikator performa akhir.

## Cleaning Data
"""

# 1. Cek & Hapus Duplicate Rows
print("\n=== Jumlah Duplicate Rows Sebelum Dihapus ===")
print("train     :", train.duplicated().sum())
print("prestasi  :", prestasi.duplicated().sum())
print("graduation:", graduation.duplicated().sum())

train = train.drop_duplicates().reset_index(drop=True)
prestasi = prestasi.drop_duplicates().reset_index(drop=True)
graduation = graduation.drop_duplicates().reset_index(drop=True)

print("\nâœ… Duplicate rows dihapus.")

# 2. Cek Missing Value
def check_missing_values(df, df_name):
    print(f"\nMissing Value pada Dataset: {df_name}")
    missing = df.isnull().sum()
    print(missing[missing > 0])

# Cek masing-masing dataset
check_missing_values(train, 'train_translated')
check_missing_values(prestasi, 'parenting_prestasi')
check_missing_values(graduation, 'graduation_rate')

# 3. Menangani Missing Value
# TRAIN
train.dropna(subset=['Response'], inplace=True)

# 4. Konfirmasi Penanganan Missing Value
# Ulang cek missing value
check_missing_values(train, 'train_translated (cleaned)')
check_missing_values(prestasi, 'parenting_prestasi (cleaned)')
check_missing_values(graduation, 'graduation_rate (cleaned)')

# Untuk prestasi, misalnya kolom 'Topic' atau 'Relation'
text_col_prestasi = 'Topic'  # ganti dengan nama kolom yang benar-benar teks

# Untuk graduation, misalnya tidak ada kolom teks asli â†’ buat kolom manual
graduation['pertanyaan'] = graduation.apply(
    lambda row: f"Student with GPA {row['college gpa']} graduated in {row['years to graduate']} years", axis=1
)
text_col_grad = 'pertanyaan'

# 6. Stopwords Indonesia sederhana (bisa diganti Sastrawi/NLTK versi lengkap)
stopwords = set([
    "yang", "dan", "di", "ke", "dari", "adalah", "itu", "untuk", "dengan", "saya",
    "kamu", "tidak", "apa", "bagaimana", "kenapa", "mengapa", "ya", "bisa",
    "karena", "jadi", "pada", "oleh", "sebagai", "mereka", "kita", "akan", "dalam"
])

# 7. Fungsi cleaning teks
def clean_text_advanced(text):
    if pd.isnull(text): return ""
    text = str(text).lower()
    text = re.sub(r"http\S+", "", text)              # hapus URL
    text = re.sub(r"@\w+", "", text)                 # hapus mention
    text = re.sub(r"#\w+", "", text)                 # hapus hashtag
    text = re.sub(r"\d+", "", text)                  # hapus angka
    text = re.sub(r"[^\x00-\x7F]+", " ", text)       # hapus emoji dan karakter non-ASCII
    text = re.sub(r"[^\w\s]", "", text)              # hapus tanda baca
    text = re.sub(r"\s+", " ", text).strip()         # hapus spasi berlebih
    tokens = text.split()
    tokens = [word for word in tokens if word not in stopwords]
    return " ".join(tokens)

# 8. Cleaning Dataset 1: train_translated.csv
if 'Context_Indo' in train.columns:
    train['cleaned_context'] = train['Context_Indo'].apply(clean_text_advanced)
else:
    raise ValueError("Kolom 'Context_Indo' tidak ditemukan di dataset train")

# 9. Cleaning Dataset 2: parenting_prestasi.csv
text_col_prestasi = 'pertanyaan' if 'pertanyaan' in prestasi.columns else prestasi.columns[0]
prestasi['cleaned_text'] = prestasi[text_col_prestasi].apply(clean_text_advanced)

# 10. Cleaning Dataset 3: graduation_rate.csv
text_col_grad = 'pertanyaan' if 'pertanyaan' in graduation.columns else graduation.columns[0]
graduation['cleaned_text'] = graduation[text_col_grad].apply(clean_text_advanced)

# 11. Cek hasil pembersihan
print("=== Contoh Cleaning Dataset train ===")
print(train[['Context_Indo', 'cleaned_context']].head(3))

print("\n=== Contoh Cleaning Dataset prestasi ===")
print(prestasi[[text_col_prestasi, 'cleaned_text']].head(3))

print("\n=== Contoh Cleaning Dataset graduation ===")
print(graduation[[text_col_grad, 'cleaned_text']].head(3))

# 12. LDA untuk Pelabelan Topik
vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
X = vectorizer.fit_transform(train['cleaned_context'])

lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(X)

# Prediksi topik untuk setiap baris
train['Topik_Prediksi'] = lda.transform(X).argmax(axis=1)

# 13. Mapping Label Topik
label_topik = {
    0: "Kesehatan Mental",
    1: "Parenting/Keluarga",
    2: "Karier/Motivasi",
    3: "Relasi Sosial",
    4: "Pendidikan"
}
train['label'] = train['Topik_Prediksi'].map(label_topik)

"""## Penggabungan Dataset Setelah Cleaning : datasetfinalcleaning"""

# Persiapan Gabung Dataset
# 1. Rename kolom teks menjadi 'text' agar seragam
train_cleaned = train[['cleaned_context', 'label']].rename(columns={'cleaned_context': 'text'})
prestasi_cleaned = prestasi[['cleaned_text']].copy()
prestasi_cleaned['label'] = 'Parenting/Keluarga'
prestasi_cleaned.rename(columns={'cleaned_text': 'text'}, inplace=True)

graduation_cleaned = graduation[['cleaned_text']].copy()
graduation_cleaned['label'] = 'Pendidikan'
graduation_cleaned.rename(columns={'cleaned_text': 'text'}, inplace=True)

# 2. Gabungkan Semua Dataset
datasetfinalcleaning = pd.concat([train_cleaned, prestasi_cleaned, graduation_cleaned], ignore_index=True)

# 3. Simpan ke CSV
datasetfinalcleaning.to_csv("datasetgabungan.csv", index=False)
print("âœ… Dataset final berhasil disimpan sebagai 'datasetgabungan.csv'")

# 5. Opsional: cek hasil gabungan
# Baca file CSV yang sudah disimpan
datasetgabungan = pd.read_csv('datasetgabungan.csv')

# Tampilkan 5 sampel acak yang rapi
print(datasetgabungan.sample(5))

# Tampilkan total data
print(f"\nTotal data setelah digabung: {len(datasetgabungan)}")

"""## Insight Dataset Setelah Cleaning
### ðŸ§¼ Deskripsi Singkat:
Dataset akhir bernama `datasetfinalcleaning` merupakan hasil penggabungan dari tiga sumber data berbeda (train_translated, parenting_prestasi, dan graduation_rate) yang telah dibersihkan melalui proses:

1. Penghapusan data **duplicate** dan **missing values**.
2. Pembersihan teks dari noise seperti URL, mention, emoji, angka, dan tanda baca.

### ðŸ§¾ Struktur Dataset

| Kolom | Deskripsi |
|-------|-----------|
| `text` | Teks hasil cleaning dari masing-masing dataset sumber. |
| `label` | Kategori/konteks dari teks, seperti hasil klasifikasi atau sumber asal data. |
"""

# Penanganan Tambahan dan Filter Ulang Data Kosong atau Terlalu Pendek
datasetgabungan = datasetgabungan[datasetgabungan['text'].str.strip().str.len() > 5]

# Menampilkan 5 sampel secara acak dalam bentuk tabel yang rapi
print(datasetgabungan.sample(10))

"""## EDA (Exploratory Data Analysis) Dataset Hasil Cleaning"""

# 1. Info dasar dataset
print("INFO DATASET:")
print(datasetgabungan.info())

# 2. Deskripsi Numerik Dataset
print("\nDESKRIPSI DATA NUMERIK:")
print(datasetgabungan.describe())

# 3. Memastikan Sudah Tidak Ada Missing Values
print("\nMissing values per kolom:")
print(datasetgabungan.isnull().sum())

# 3. Statistik panjang teks
# Hapus karakter tak terlihat dari label
datasetgabungan['label'] = datasetgabungan['label'].str.replace(r'[\r\n\t\x0b\x0c\x13]', '', regex=True)
datasetgabungan['text'] = datasetgabungan['text'].str.replace(r'[\r\n\t\x0b\x0c\x13]', ' ', regex=True)

# Tambahkan kolom panjang teks
datasetgabungan['text_length'] = datasetgabungan['text'].astype(str).str.len()

# Cek Statistik Panjang Teks
datasetgabungan['text_length'] = datasetgabungan['text'].astype(str).str.len()
print("\nStatistik panjang teks:")
print(datasetgabungan['text_length'].describe())

# 4. Visualisasi distribusi panjang teks
plt.figure(figsize=(10,5))
sns.histplot(datasetgabungan['text_length'], bins=50, kde=True)
plt.title('Distribusi Panjang Teks')
plt.xlabel('Panjang Teks (karakter)')
plt.ylabel('Frekuensi')
plt.show()

# 5. Distribusi kelas/label
plt.figure(figsize=(8,4))
sns.countplot(x='label', data=datasetgabungan, order=datasetgabungan['label'].value_counts().index)
plt.title('Distribusi Label Kelas')
plt.xlabel('Label')
plt.ylabel('Jumlah Data')
plt.show()

# 6. Boxplot panjang teks per label
plt.figure(figsize=(10,6))
sns.boxplot(x='label', y='text_length', data=datasetgabungan)
plt.title('Distribusi Panjang Teks per Label')
plt.xlabel('Label')
plt.ylabel('Panjang Teks (karakter)')
plt.show()

# 7. Contoh teks terpanjang dan terpendek per label
print("\nContoh teks terpanjang dan terpendek per label:")
for label in datasetgabungan['label'].unique():
    subset = datasetgabungan[datasetgabungan['label'] == label]
    longest = subset.loc[subset['text_length'].idxmax()]
    shortest = subset.loc[subset['text_length'].idxmin()]
    print(f"\nLabel: {label}")
    print(f" - Teks terpanjang ({longest['text_length']} karakter): {longest['text'][:200]}...")
    print(f" - Teks terpendek ({shortest['text_length']} karakter): {shortest['text'][:200]}...")

# 8. Frekuensi kata umum (optional, pakai CountVectorizer dari sklearn)
print("\nTop 10 kata paling sering muncul (semua data):")
vectorizer = CountVectorizer(stop_words='english', max_features=10)
X = vectorizer.fit_transform(datasetgabungan['text'])
freq = X.toarray().sum(axis=0)
freq_df = pd.DataFrame({'kata': vectorizer.get_feature_names_out(), 'frekuensi': freq})
print(freq_df.sort_values(by='frekuensi', ascending=False))

"""## Ekstraksi Fitur (TF-IDF atau Count Vectorizer)"""

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X = vectorizer.fit_transform(datasetgabungan['text'])

# Encode Label
le = LabelEncoder()
y = le.fit_transform(datasetgabungan['label'])

"""## Split Dataset Hasil Cleaning (DatasetFinal) 80:20"""

datasetfinal = datasetgabungan[datasetgabungan['text'].str.strip() != ""]

X = datasetfinal['text']
y = le.fit_transform(datasetfinal['label'])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

"""# Simulasi Data Realistis ParentingMatch

Disimulasikan 500 responden yang menjawab 10 pertanyaan parenting (skala 1â€“5).
Setiap jawaban dijumlahkan menjadi skor total (sum).
Label ditentukan berdasarkan skor total:

- â‰¥ 40 â†’ Authoritative
- 25 - 39 â†’ Permissive
- < 25 â†’ Authoritarian

Tujuan pembagian ini adalah mensimulasikan real-case kuisioner parenting style.
- Output: DataFrame dengan 10 kolom pertanyaan dan satu kolom label (Authoritative, dll).
"""

# Buat data dummy 10 fitur
np.random.seed(42)
n_samples = 1000
data = []

for _ in range(n_samples):
    scores = np.random.randint(1, 6, size=10).tolist()  # skor 1â€“5

    # Ambil nilai total untuk tiap gaya
    group = {
        "Authoritative": sum(scores[:3]),
        "Permissive": sum(scores[3:6]),
        "Democratic": sum(scores[6:8]),
        "Neglectful": sum(scores[8:]),
    }

    max_val = max(group.values())
    top_styles = [k for k, v in group.items() if v == max_val]

    label = "Mixed" if len(top_styles) > 1 else top_styles[0]

    data.append(scores + [label])

# DataFrame
columns = [f"q{i+1}" for i in range(10)] + ["Label"]
df = pd.DataFrame(data, columns=columns)
df.head()

"""# Encode label

##### Label (string) dikodekan ke dalam angka (0, 1, 2) dengan LabelEncoder agar bisa diproses oleh algoritma ML.
##### Dataset kemudian dibagi menjadi:

- 80%: Data latih (X_train, y_train)
- 20%: Data uji (X_test, y_test)
"""

le = LabelEncoder()
df["Label_encoded"] = le.fit_transform(df["Label"])

X = df.drop(columns=["Label", "Label_encoded"])
y = df["Label_encoded"]

# Split dan latih model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Modeling dengan Random Forest

- Menggunakan Random Forest Classifier, cocok untuk klasifikasi multi-kelas dan robust terhadap overfitting.
- n_estimators=100: jumlah pohon dalam hutan acak.
"""

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

"""# Evaluasi Model

##### Evaluasi dilakukan dengan:

- Classification Report (precision, recall, f1-score per kelas)
- Confusion Matrix untuk melihat prediksi vs kenyataan secara visual.

Hasil ini akan menunjukkan apakah model bisa mengenali gaya parenting dengan baik berdasarkan jawaban kuis.
"""

y_pred = model.predict(X_test)
present_labels = np.unique(y_test)
present_classes = le.inverse_transform(present_labels)

print(classification_report(y_test, y_pred, target_names=present_classes))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=present_classes, yticklabels=present_classes)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# Simpan Model

- Model dan LabelEncoder disimpan dalam satu file .pkl.
- File ini nantinya bisa digunakan untuk integrasi ke backend (misal: PHP native atau Next.js API).
"""

joblib.dump(le, "ParentCare-BE/label_parenting_match_model.pkl")
joblib.dump(model, 'ParentCare-BE/parenting_match_model.pkl')
with open("label_parenting_match_model.pkl", "rb") as f1, open("parenting_match_model.pkl", "rb") as f2:
    response = requests.post(
        "https://be-production-0885.up.railway.app/api/model/upload",
        files={
            "label_model": f1,
            "main_model": f2
        }
    )
print("Model disimpan sebagai parenting_match_model.pkl")